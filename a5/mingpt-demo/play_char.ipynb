{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a character-level GPT on some text data\n",
    "\n",
    "The inputs here are simple text files, which we chop up to individual characters and then train GPT on. So you could say this is a char-transformer instead of a char-rnn. Doesn't quite roll off the tongue as well. In this example we will feed it some Shakespeare, which we'll get it to predict character-level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-09T21:25:47.556172Z",
     "start_time": "2023-05-09T21:25:47.554620Z"
    }
   },
   "outputs": [],
   "source": [
    "# set up logging\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-09T21:25:47.918261Z",
     "start_time": "2023-05-09T21:25:47.913746Z"
    }
   },
   "outputs": [],
   "source": [
    "# make deterministic\n",
    "from mingpt.utils import set_seed\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-09T21:25:48.189754Z",
     "start_time": "2023-05-09T21:25:48.187064Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-09T21:25:48.386229Z",
     "start_time": "2023-05-09T21:25:48.383914Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CharDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data, block_size):\n",
    "        chars = sorted(list(set(data)))\n",
    "        data_size, vocab_size = len(data), len(chars)\n",
    "        print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "        \n",
    "        self.stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "        self.itos = { i:ch for i,ch in enumerate(chars) }\n",
    "        self.block_size = block_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.data = data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.block_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # grab a chunk of (block_size + 1) characters from the data\n",
    "        chunk = self.data[idx:idx + self.block_size + 1]\n",
    "        # encode every character to an integer\n",
    "        dix = [self.stoi[s] for s in chunk]\n",
    "        \"\"\"\n",
    "        arrange data and targets so that the first i elements of x\n",
    "        will be asked to predict the i-th element of y. Notice that\n",
    "        the eventual language model will actually make block_size\n",
    "        individual predictions at the same time based on this data,\n",
    "        so we are being clever and amortizing the cost of the forward\n",
    "        pass of the network. So for example if block_size is 4, then\n",
    "        we could e.g. sample a chunk of text \"hello\", the integers in\n",
    "        x will correspond to \"hell\" and in y will be \"ello\". This will\n",
    "        then actually \"multitask\" 4 separate examples at the same time\n",
    "        in the language model:\n",
    "        - given just \"h\", please predict \"e\" as next\n",
    "        - given \"he\" please predict \"l\" next\n",
    "        - given \"hel\" predict \"l\" next\n",
    "        - given \"hell\" predict \"o\" next\n",
    "        \n",
    "        In addition, because the DataLoader will create batches of examples,\n",
    "        every forward/backward pass during traning will simultaneously train\n",
    "        a LOT of predictions, amortizing a lot of computation. In particular,\n",
    "        for a batched input of integers X (B, T) where B is batch size and\n",
    "        T is block_size and Y (B, T), the network will during training be\n",
    "        simultaneously training to make B*T predictions, all at once! Of course,\n",
    "        at test time we can paralellize across batch B, but unlike during training\n",
    "        we cannot parallelize across the time dimension T - we have to run\n",
    "        a forward pass of the network to recover the next single character of the \n",
    "        sequence along each batch dimension, and repeatedly always feed in a next\n",
    "        character to get the next one.\n",
    "        \n",
    "        So yes there is a big asymmetry between train/test time of autoregressive\n",
    "        models. During training we can go B*T at a time with every forward pass,\n",
    "        but during test time we can only go B at a time, T times, with T forward \n",
    "        passes.\n",
    "        \"\"\"\n",
    "        x = torch.tensor(dix[:-1], dtype=torch.long)\n",
    "        y = torch.tensor(dix[1:], dtype=torch.long)\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-09T21:25:48.876440Z",
     "start_time": "2023-05-09T21:25:48.874141Z"
    }
   },
   "outputs": [],
   "source": [
    "block_size = 128 # spatial extent of the model for its context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-09T21:25:49.214206Z",
     "start_time": "2023-05-09T21:25:49.200481Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "data has 1115394 characters, 65 unique.\n",
      "1115394\n"
     ]
    }
   ],
   "source": [
    "# you can download this file at https://github.com/karpathy/char-rnn/blob/master/data/tinyshakespeare/input.txt\n",
    "text = open('input.txt', 'r').read() # don't worry we won't run out of file handles\n",
    "print(type(text))\n",
    "train_dataset = CharDataset(text, block_size) # one line of poem is roughly 50 characters\n",
    "print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-09T21:25:49.875792Z",
     "start_time": "2023-05-09T21:25:49.530286Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/09/2023 17:25:49 - INFO - mingpt.model -   number of parameters: 2.535219e+07\n"
     ]
    }
   ],
   "source": [
    "from mingpt.model import GPT, GPTConfig\n",
    "mconf = GPTConfig(train_dataset.vocab_size, train_dataset.block_size,\n",
    "                  n_layer=8, n_head=8, n_embd=512)\n",
    "model = GPT(mconf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-09T21:26:05.475141Z",
     "start_time": "2023-05-09T21:25:51.576515Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/Users/andrewcheng/anaconda3/lib/python3.10/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/Users/andrewcheng/anaconda3/lib/python3.10/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'CharDataset' on <module '__main__' (built-in)>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[34], line 8\u001B[0m\n\u001B[1;32m      4\u001B[0m tconf \u001B[38;5;241m=\u001B[39m TrainerConfig(max_epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m, batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m512\u001B[39m, learning_rate\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m6e-4\u001B[39m,\n\u001B[1;32m      5\u001B[0m                       lr_decay\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, warmup_tokens\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m512\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m20\u001B[39m, final_tokens\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mlen\u001B[39m(train_dataset)\u001B[38;5;241m*\u001B[39mblock_size,\n\u001B[1;32m      6\u001B[0m                       num_workers\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m4\u001B[39m)\n\u001B[1;32m      7\u001B[0m trainer \u001B[38;5;241m=\u001B[39m Trainer(model, train_dataset, \u001B[38;5;28;01mNone\u001B[39;00m, tconf)\n\u001B[0;32m----> 8\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/CS224n-projects/a5/mingpt-demo/mingpt/trainer.py:122\u001B[0m, in \u001B[0;36mTrainer.train\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    119\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtokens \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;66;03m# counter used for learning rate decay\u001B[39;00m\n\u001B[1;32m    120\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(config\u001B[38;5;241m.\u001B[39mmax_epochs):\n\u001B[0;32m--> 122\u001B[0m     \u001B[43mrun_epoch\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtrain\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    123\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtest_dataset \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    124\u001B[0m         test_loss \u001B[38;5;241m=\u001B[39m run_epoch(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtest\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[0;32m~/PycharmProjects/CS224n-projects/a5/mingpt-demo/mingpt/trainer.py:73\u001B[0m, in \u001B[0;36mTrainer.train.<locals>.run_epoch\u001B[0;34m(split)\u001B[0m\n\u001B[1;32m     68\u001B[0m loader \u001B[38;5;241m=\u001B[39m DataLoader(data, shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, pin_memory\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m     69\u001B[0m                     batch_size\u001B[38;5;241m=\u001B[39mconfig\u001B[38;5;241m.\u001B[39mbatch_size,\n\u001B[1;32m     70\u001B[0m                     num_workers\u001B[38;5;241m=\u001B[39mconfig\u001B[38;5;241m.\u001B[39mnum_workers)\n\u001B[1;32m     72\u001B[0m losses \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m---> 73\u001B[0m pbar \u001B[38;5;241m=\u001B[39m tqdm(\u001B[38;5;28;43menumerate\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mloader\u001B[49m\u001B[43m)\u001B[49m, total\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mlen\u001B[39m(loader)) \u001B[38;5;28;01mif\u001B[39;00m is_train \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(loader)\n\u001B[1;32m     74\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m it, (x, y) \u001B[38;5;129;01min\u001B[39;00m pbar:\n\u001B[1;32m     75\u001B[0m \n\u001B[1;32m     76\u001B[0m     \u001B[38;5;66;03m# place data on the correct device\u001B[39;00m\n\u001B[1;32m     77\u001B[0m     x \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice)\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.10/site-packages/torch/utils/data/dataloader.py:444\u001B[0m, in \u001B[0;36mDataLoader.__iter__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    442\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_iterator\n\u001B[1;32m    443\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 444\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_iterator\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.10/site-packages/torch/utils/data/dataloader.py:390\u001B[0m, in \u001B[0;36mDataLoader._get_iterator\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    388\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    389\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcheck_worker_number_rationality()\n\u001B[0;32m--> 390\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_MultiProcessingDataLoaderIter\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1077\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter.__init__\u001B[0;34m(self, loader)\u001B[0m\n\u001B[1;32m   1070\u001B[0m w\u001B[38;5;241m.\u001B[39mdaemon \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m   1071\u001B[0m \u001B[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001B[39;00m\n\u001B[1;32m   1072\u001B[0m \u001B[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001B[39;00m\n\u001B[1;32m   1073\u001B[0m \u001B[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001B[39;00m\n\u001B[1;32m   1074\u001B[0m \u001B[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001B[39;00m\n\u001B[1;32m   1075\u001B[0m \u001B[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001B[39;00m\n\u001B[1;32m   1076\u001B[0m \u001B[38;5;66;03m#     AssertionError: can only join a started process.\u001B[39;00m\n\u001B[0;32m-> 1077\u001B[0m \u001B[43mw\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstart\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1078\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_index_queues\u001B[38;5;241m.\u001B[39mappend(index_queue)\n\u001B[1;32m   1079\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_workers\u001B[38;5;241m.\u001B[39mappend(w)\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.10/multiprocessing/process.py:121\u001B[0m, in \u001B[0;36mBaseProcess.start\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    118\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m _current_process\u001B[38;5;241m.\u001B[39m_config\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdaemon\u001B[39m\u001B[38;5;124m'\u001B[39m), \\\n\u001B[1;32m    119\u001B[0m        \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdaemonic processes are not allowed to have children\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m    120\u001B[0m _cleanup()\n\u001B[0;32m--> 121\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_popen \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_Popen\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    122\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sentinel \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_popen\u001B[38;5;241m.\u001B[39msentinel\n\u001B[1;32m    123\u001B[0m \u001B[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001B[39;00m\n\u001B[1;32m    124\u001B[0m \u001B[38;5;66;03m# reference to the process object (see bpo-30775)\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.10/multiprocessing/context.py:224\u001B[0m, in \u001B[0;36mProcess._Popen\u001B[0;34m(process_obj)\u001B[0m\n\u001B[1;32m    222\u001B[0m \u001B[38;5;129m@staticmethod\u001B[39m\n\u001B[1;32m    223\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_Popen\u001B[39m(process_obj):\n\u001B[0;32m--> 224\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_default_context\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_context\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mProcess\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_Popen\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprocess_obj\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.10/multiprocessing/context.py:288\u001B[0m, in \u001B[0;36mSpawnProcess._Popen\u001B[0;34m(process_obj)\u001B[0m\n\u001B[1;32m    285\u001B[0m \u001B[38;5;129m@staticmethod\u001B[39m\n\u001B[1;32m    286\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_Popen\u001B[39m(process_obj):\n\u001B[1;32m    287\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpopen_spawn_posix\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Popen\n\u001B[0;32m--> 288\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mPopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprocess_obj\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.10/multiprocessing/popen_spawn_posix.py:32\u001B[0m, in \u001B[0;36mPopen.__init__\u001B[0;34m(self, process_obj)\u001B[0m\n\u001B[1;32m     30\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, process_obj):\n\u001B[1;32m     31\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fds \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m---> 32\u001B[0m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mprocess_obj\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.10/multiprocessing/popen_fork.py:19\u001B[0m, in \u001B[0;36mPopen.__init__\u001B[0;34m(self, process_obj)\u001B[0m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreturncode \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     18\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfinalizer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m---> 19\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_launch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprocess_obj\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.10/multiprocessing/popen_spawn_posix.py:62\u001B[0m, in \u001B[0;36mPopen._launch\u001B[0;34m(self, process_obj)\u001B[0m\n\u001B[1;32m     60\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msentinel \u001B[38;5;241m=\u001B[39m parent_r\n\u001B[1;32m     61\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(parent_w, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mwb\u001B[39m\u001B[38;5;124m'\u001B[39m, closefd\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m) \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[0;32m---> 62\u001B[0m         \u001B[43mf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwrite\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgetbuffer\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     63\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m     64\u001B[0m     fds_to_close \u001B[38;5;241m=\u001B[39m []\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from mingpt.trainer import Trainer, TrainerConfig\n",
    "\n",
    "# initialize a trainer instance and kick off training\n",
    "tconf = TrainerConfig(max_epochs=2, batch_size=512, learning_rate=6e-4,\n",
    "                      lr_decay=True, warmup_tokens=512*20, final_tokens=2*len(train_dataset)*block_size,\n",
    "                      num_workers=4)\n",
    "trainer = Trainer(model, train_dataset, None, tconf)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-09T21:26:05.471695Z"
    }
   },
   "outputs": [],
   "source": [
    "# alright, let's sample some character-level Shakespeare\n",
    "from mingpt.utils import sample\n",
    "\n",
    "context = \"O God, O God!\"\n",
    "x = torch.tensor([train_dataset.stoi[s] for s in context], dtype=torch.long)[None,...].to(trainer.device)\n",
    "y = sample(model, x, 2000, temperature=1.0, sample=True, top_k=10)[0]\n",
    "completion = ''.join([train_dataset.itos[int(i)] for i in y])\n",
    "print(completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-09T21:26:05.474177Z"
    }
   },
   "outputs": [],
   "source": [
    "len(completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "torch.cuda.current_device()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
